Ex1:
	When running a parallel program, the threads are assigned to different processing cores of the CPU, and each thread executes its portion of the code independently of the others. The order in which the threads execute and complete their tasks is determined by the scheduling policy used by the OpenMP implementation.
	
Ex2:
	Slicing:First get the thread ID and the total number of threads using omp_get_thread_num() and omp_get_num_threads(). Then use tid to determine which rows to compute. Specifically, thread 0 will compute the rows at indices i such that i % num_threads is 0, thread 1 will compute the rows where i % num_threads is 1, and so on.

	Chunking:Use omp_get_num_threads() to get the number of threads currently executing the parallel region, and then divide the rows of matrix a into that many contiguous chunks, each of size MATRIX_SIZE/omp_get_num_threads(). We use the parallel for directive to distribute these chunks across the threads for parallel computation. The loop inside the parallel for directive iterates over the rows of the chunk assigned to the current thread, and computes the corresponding rows of matrix c using the entire matrix b.
	
	1.Scalability limitations: OpenMP is designed for shared-memory parallelism, which means that it is best suited for systems with a small number of cores, typically up to a few tens of cores. Beyond that, the scalability of OpenMP may be limited by factors such as memory bandwidth, cache contention, and communication overhead.
	2.Load balancing issues: In some cases, OpenMP may not distribute the workload evenly across all the cores, leading to load balancing issues. For example, if some parts of the code are more computationally intensive than others, some cores may be idle while others are overburdened, leading to suboptimal performance.
	3.Communication overhead: OpenMP relies on shared memory for inter-thread communication, which can be a bottleneck if the threads need to communicate frequently or if the data they are sharing is large. In some cases, using a message-passing interface such as MPI may be more efficient.
	4.Compiler limitations: The performance of OpenMP code can be highly dependent on the compiler used to compile the code. Different compilers may generate different code optimizations or have different support for OpenMP features, leading to differences in performance.

Ex3:
	
